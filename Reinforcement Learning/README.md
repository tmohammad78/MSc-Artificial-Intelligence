# RL
RL is my favorite and It has a long history, There are so resources for learning RL but someone them are bullshit and waste your time. I found some resources that are better. <br />

**Note: All of the text was reviewed by myself, but some parts, due to saving time, were generated by ChatGPT**<br />
The key players in Reinforcement Learning (RL) are the agent and the environment. The environment represents the world in which the agent exists and interacts with. During each interaction, the agent perceives the current state of the world through observation, which may be partial, and then chooses an action to take. The environment changes in response to the agent's actions and can also change independently.

The agent receives a reward signal from the environment, a numerical value that indicates the quality of the current state. The objective of the agent is to maximize its cumulative reward, known as the return. Reinforcement Learning techniques allow the agent to learn and develop behaviors to achieve its goal of maximizing the return.

![RL](https://github.com/tmohammad78/MSc-Artificial-Intelligence/blob/main/Reinforcement%20Learning/images/agent.png)
## Components of RL
### Agent: 
The agent is the decision-maker in the RL system. It takes actions in the environment and receives feedback in the form of rewards.

### Environment: 
The environment represents the world in which the agent operates. It defines the state space, the action space, and the reward structure.

### State:
The state represents the current situation or condition of the environment. It can be partial or fully observable.

### Action: 
The action represents the choices available to the agent at a given time step.

### Reward:
The reward is a scalar value that represents the feedback given to the agent after taking an action. The goal of the agent is to maximize the cumulative reward over time.

### Policy:
The policy defines the mapping from states to actions. It represents the behavior of the agent.
A deterministic policy is a function that maps states to a unique action. This means that the agent will always take the same action in a given state, regardless of the context. Deterministic policies are easy to implement and can be used in simple environments with limited state and action spaces.

<span style="color:lightblue;">
A <b>deterministic policy </b> is a function that maps states to a unique action. This means that the agent will always take the same action in a given state, regardless of the context. Deterministic policies are easy to implement and can be used in simple environments with limited state and action spaces.
</span>

<span style="color:pink;">
A <b>stochastic policy</b> is a function that maps states to a probability distribution over actions. This means that the agent has a probability of taking each possible action in a given state. Stochastic policies are more flexible than deterministic policies and can be used in more complex environments where the optimal action is not always clear. Stochastic policies can also help the agent explore the environment and find the best action to take in each state.
</span>

In practice, most policy functions in RL are parameterized, meaning that they are represented as a function with a set of parameters that can be optimized to produce the best behavior. Some common methods for parameterizing policies include linear policies, neural networks, and Gaussian processes.

### Value function:
The value function represents the expected cumulative reward for a given state or state-action pair. It is used by the agent to evaluate the quality of its decisions.

### Model:
The model is a representation of the environment's dynamics. It can be used to simulate the environment and estimate the consequences of actions.
Reinforcement Learning (RL) algorithms can be broadly categorized into two categories: model-based and model-free.

<span style="color:orange;">
A <b>model-based</b> agent is one that has a model of the environment, including the state transition and reward functions. The agent uses this model to simulate future states and estimate the expected rewards of taking certain actions. Model-based agents can be more accurate in their predictions, but they require more memory and computational resources to store and manipulate the model.
</span>
<br>------------------------------------------------------ <br />
<span style="color:orange;">
A model-free agent, on the other hand, does not have an explicit model of the environment. Instead, it relies on experiences and learned patterns to make decisions. Model-free algorithms are typically simpler and faster, but they may have a harder time adapting to changing environments.
</span>
A real-world example of a model-based agent could be an autonomous vehicle that uses a map and sensor data to determine its location and plan a path. It uses its internal model of the environment to predict traffic patterns and avoid obstacles.

A real-world example of a model-free agent could be a deep reinforcement learning agent trained to play video games. It does not have an explicit model of the game environment, but instead relies on trial and error and learned patterns to make decisions.

It is worth noting that most RL algorithms use a combination of model-based and model-free elements, and the distinction between the two can be somewhat blurry.
## Resources
https://www.youtube.com/watch?v=2nKD6zFQ8xI <br />
https://www.youtube.com/watch?v=Psrhxy88zww <br />
Deep mind course on youtube <br />
https://www.davidsilver.uk/teaching/ <br />
https://stable-baselines.readthedocs.io/en/master/index.html <br />
https://spinningup.openai.com/ <br />
Bootcamp : https://sites.google.com/view/deep-rl-bootcamp/lectures?pli=1 <br />
https://imitation.readthedocs.io/ <br />
Rail Page <br />